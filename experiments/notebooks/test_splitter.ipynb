{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_splitter import CustomTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CustomTextSplitter(chunk_size=1024, chunk_overlap=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885\n",
      "#Big-Data\n",
      "\n",
      "Это [[Брокер сообщений]], распределенная платформа для потоковой обработки событий, обмена сообщениями в режиме реального времени для реализации [[Event Driven Architecture]]\n",
      "Еще её называют распределенным логом, так как данные пишутся в конец очереди не модифицируются\n",
      "Kafka пассивна и максимально глупая, но производительная\n",
      "### Основные термины\n",
      "\n",
      "**Producer** - сервис, который поставляет сообщения в очередь\n",
      "**Consumer** - сервис, который потребляет сообщения из очереди\n",
      "**Event** - сообщение, имеющее следующую структуру:\n",
      "- Ключ - идентификатор сообщения\n",
      "- Значение - полезная нагрузка в байтах\n",
      "- Таймстамп - временная отметка появления сообщения в брокере\n",
      "**Topic** - место, в котором хранятся однородные сообщения, является логическим разделением данных в Kafka. Может быть разделено на несколько партиций\n",
      "**Partition** - часть топика, где физически хранятся сообщения\n",
      "========================================\n",
      "838\n",
      "### Партиционирование \n",
      "\n",
      "Топик делится на полностью независимые партиции, благодаря чему обеспечивается горизонтальное масштабирование Kafka. Количество партиций задается при создании топика. \n",
      "Оптимальным с точки зрения эффективности числом партиций является число консьюмеров, однако хорошей практикой является делать по 4 партиции на консьюмера. В этом случае, если нужно будет ускоряться, то просто добавим консьюмеров. Добавить партицию в существующий топик может быть проблематично с точки зрения консистентности\n",
      "Порядок выполнения сообщений сохраняется только в рамках партиции, поэтому рекомендуется устанавливать ключ партиционирования например идентификатор клиента\n",
      "Хорошей практикой является задать изначально 8 партиций для топика\n",
      "К любому сообщению можно обратиться используя номер партиции и смещение сообщения в этой партиции\n",
      "========================================\n",
      "955\n",
      "### Реплицируемость\n",
      "\n",
      "По умолчанию создается один топик на всю систему, однако можно задать фактор репликации, чтобы одновременно существовало несколько одинаковых топиков, что повышает отказоустойчивость\n",
      "Данные идут на мастер-партицию, затем они копируются на слейв-партиции согласно фактору репликации. Время синхронизации порядка 10 мс. Реплики не взаимодействуют с консьюмерами, они нужны только как бэкапы для быстрого восстановления после падения мастера\n",
      "Хорошей практикой является использование 3 реплик (если данные важны), для аналитики можно и без реплик\n",
      "\n",
      "### Потеря сообщений\n",
      "\n",
      "Реплики все равно запаздывают по данным от мастера. Если меняется мастер, например из-за сбоя, то в качестве нового мастера может быть выбран слейв, на котором меньше сообщений чем на предыдущем мастере. В этом случае все слейвы, у которых были эти сообщения, будут вынуждены их затереть\n",
      "От этого никак не обезопаситься, но такое случается редко\n",
      "\n",
      "## Особенности работы\n",
      "========================================\n",
      "715\n",
      "### Потеря сообщений\n",
      "\n",
      "Реплики все равно запаздывают по данным от мастера. Если меняется мастер, например из-за сбоя, то в качестве нового мастера может быть выбран слейв, на котором меньше сообщений чем на предыдущем мастере. В этом случае все слейвы, у которых были эти сообщения, будут вынуждены их затереть\n",
      "От этого никак не обезопаситься, но такое случается редко\n",
      "\n",
      "## Особенности работы\n",
      "### Настройки продьюсера\n",
      "\n",
      "- протокол сериализации (JSON, Avro, Protobuf)\n",
      "- максимальный размер батча перед отправкой: количество сообщений или размер батча в байтах\n",
      "- таймапут отправки батча (т.к. нужно количество сообщений может не успеть накопиться)\n",
      "- повторная отправка батча, если брокер в момент отправки был недоступен\n",
      "========================================\n",
      "1001\n",
      "## Особенности работы\n",
      "### Настройки продьюсера\n",
      "\n",
      "- протокол сериализации (JSON, Avro, Protobuf)\n",
      "- максимальный размер батча перед отправкой: количество сообщений или размер батча в байтах\n",
      "- таймапут отправки батча (т.к. нужно количество сообщений может не успеть накопиться)\n",
      "- повторная отправка батча, если брокер в момент отправки был недоступен\n",
      "### Настройки консьюмера\n",
      "\n",
      "- минимальное количество байт для получения данных из брокера\n",
      "- автокоммит - консьюмер забирает данные из брокера и говорит о том, что он их условно сделал и на стороне брокера происходит изменение смещения. Но если консьюмер отвалится, то он не вернется к тем сообщения, потому офсет уже другой. Если нужен более надежный вариант, то нужно коммитить вручную и увеличивать таймаут сессии, чтобы кафка не разорвала соединение\n",
      "\n",
      "### Если сообщение доставилось\n",
      "\n",
      "- перестать слать сообщения\n",
      "- отправлять их в dead letter queue\n",
      "- отправлять их в retry queue - после которой есть сервис выполняющий повторные отправки в целевую очередь\n",
      "========================================\n",
      "319\n",
      "### Если сообщение доставилось\n",
      "\n",
      "- перестать слать сообщения\n",
      "- отправлять их в dead letter queue\n",
      "- отправлять их в retry queue - после которой есть сервис выполняющий повторные отправки в целевую очередь\n",
      "\n",
      "### Инструменты\n",
      "\n",
      "- Библиотеки для Go: kafka-go, confluence-kafka-go\n",
      "- Просмотр содержимого брокера: Offset Explorer\n",
      "========================================\n",
      "948\n",
      "### Инструменты\n",
      "\n",
      "- Библиотеки для Go: kafka-go, confluence-kafka-go\n",
      "- Просмотр содержимого брокера: Offset Explorer \n",
      "\n",
      "### Кейсы применения\n",
      "\n",
      "- Лог событий для стека Big Data - приложение шлет в брокер события, события с некоторой периодичностью записывают на HDFS и затем анализируются с помощью Apache Spark\n",
      "\t- Gobblin - инструмент для загрузки данных из Apache Kafka в HDFS\n",
      "\t- Spark-Streaming - потоковая обработка на Apache Spark в режиме реального времени\n",
      "- Подписка на сообщения - приложение шлет сообщения в брокер, другое приложение их  считывает и производит обработку, таких считывателей может быть несколько. Если продьюсер упал, то он сможет подняться и спокойно перечитать сообщения, так как они хранятся в Kafka. Не следует создавать отдельный топик для каждого типа сообщения, лучше использовать единый топик для нескольких типов сообщений, а дальше на стороне продьюсера определять логику обработки только нужных сообщений \n",
      "\n",
      "## Запуск\n",
      "========================================\n",
      "665\n",
      "### Docker Compose\n",
      "\n",
      "```YAML\n",
      "version: \"3.9\"\n",
      "services:\n",
      "  zookeeper:\n",
      "    image: confluentinc/cp-zookeeper:latest\n",
      "    environment:\n",
      "      ZOOKEEPER_CLIENT_PORT: 2181\n",
      "      ZOOKEEPER_TICK_TIME: 2000\n",
      "    ports:\n",
      "      - 22181:2181\n",
      "\n",
      "  kafka:\n",
      "    image: confluentinc/cp-kafka:latest\n",
      "    depends_on:\n",
      "      - zookeeper\n",
      "    ports:\n",
      "      - 29092:29092\n",
      "    hostname: kafka\n",
      "    environment:\n",
      "      KAFKA_BROKER_ID: 1\n",
      "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n",
      "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092\n",
      "      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT\n",
      "      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n",
      "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2\n",
      "========================================\n",
      "885\n",
      "kafka:\n",
      "    image: confluentinc/cp-kafka:latest\n",
      "    depends_on:\n",
      "      - zookeeper\n",
      "    ports:\n",
      "      - 29092:29092\n",
      "    hostname: kafka\n",
      "    environment:\n",
      "      KAFKA_BROKER_ID: 1\n",
      "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n",
      "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092\n",
      "      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT\n",
      "      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n",
      "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2\n",
      "\n",
      "  kafka2:\n",
      "    image: confluentinc/cp-kafka:latest\n",
      "    depends_on:\n",
      "      - zookeeper\n",
      "    ports:\n",
      "      - 29093:29092\n",
      "    hostname: kafka2\n",
      "    environment:\n",
      "      KAFKA_BROKER_ID: 2\n",
      "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n",
      "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka2:29093\n",
      "      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT\n",
      "      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n",
      "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2\n",
      "========================================\n",
      "799\n",
      "kafka2:\n",
      "    image: confluentinc/cp-kafka:latest\n",
      "    depends_on:\n",
      "      - zookeeper\n",
      "    ports:\n",
      "      - 29093:29092\n",
      "    hostname: kafka2\n",
      "    environment:\n",
      "      KAFKA_BROKER_ID: 2\n",
      "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n",
      "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka2:29093\n",
      "      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT\n",
      "      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n",
      "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2\n",
      "\n",
      "  kafka-ui:\n",
      "    image: provectuslabs/kafka-ui\n",
      "    container_name: kafka-ui\n",
      "    ports:\n",
      "      - 8090:8080\n",
      "    restart: always\n",
      "    environment:\n",
      "      - KAFKA_CLUSTERS_0_NAME=local\n",
      "      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:29092,kafka2:29093\n",
      "      - KAFKA_CLUSTERS_0_ZOOKEEPER=zookeeper:2181\n",
      "    links:\n",
      "      - kafka\n",
      "      - kafka2\n",
      "      - zookeeper\n",
      "```\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/KnowledgeStore/Apache Kafka.md\", 'r') as file:\n",
    "    content = file.read()\n",
    "    for item in text_splitter.split_text(content):\n",
    "        print(len(item))\n",
    "        print(item)\n",
    "        print('====' * 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
